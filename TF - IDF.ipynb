{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequancy - Inverse Document Frequancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with scoring word frequency is that highly frequent words start to dominate in the document, but may not contain as much ***informational content*** to the model as rarer but perhaps domain specific words.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like “the” that are also frequent across all documents are penalized.\n",
    "\n",
    "This approach to scoring is called ***Term Frequency – Inverse Document Frequency***, or TF-IDF for short, where:\n",
    "\n",
    " * Term Frequency: is a scoring of the frequency of the word in the current document.\n",
    " * Inverse Document Frequency: is a scoring of how rare the word is across documents.\n",
    " * The scores are a weighting where not all words are equally as important or interesting.\n",
    "\n",
    "\n",
    "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"\"\"Perhaps one of the most significant advances in  made by Arabic mathematics began at this time with the work of al-Khwarizmi, namely the beginnings of algebra. It is important to understand just how significant this new idea was. It was a revolutionary move away from the Greek concept of mathematics which was essentially geometry. Algebra was a unifying theory which allowedrational numbers,irrational numbers, geometrical magnitudes, etc., to all be treated as \\\"algebraic objects\\\". It gave mathematics a whole new development path so much broader in concept to that which had existed before, and provided a vehicle for future development of the subject. Another important aspect of the introduction of algebraic ideas was that it allowed mathematics to be applied to itselfin a way which had not happened before.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perhaps one of the most significant advances in  made by Arabic mathematics began at this time with the work of al-Khwarizmi, namely the beginnings of algebra. It is important to understand just how significant this new idea was. It was a revolutionary move away from the Greek concept of mathematics which was essentially geometry. Algebra was a unifying theory which allowedrational numbers,irrational numbers, geometrical magnitudes, etc., to all be treated as \"algebraic objects\". It gave mathematics a whole new development path so much broader in concept to that which had existed before, and provided a vehicle for future development of the subject. Another important aspect of the introduction of algebraic ideas was that it allowed mathematics to be applied to itselfin a way which had not happened before.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_text =u\"\"\"ربما كانت أحد أهم التطورات التي قامت بها الرياضيات العربية التي بدأت في هذا الوقت بعمل الخوارزمي وهي بدايات الجبر, ومن المهم فهم كيف كانت هذه الفكرة الجديدة مهمة, فقد كانت خطوة نورية بعيدا عن المفهوم اليوناني للرياضيات التي هي في جوهرها هندسة, الجبر کان نظرية موحدة تتيح الأعداد الكسرية والأعداد اللا كسرية, والمقادير الهندسية وغيرها, أن تتعامل على أنها أجسام جبرية, وأعطت الرياضيات ككل مسارا جديدا للتطور بمفهوم أوسع بكثير من الذي كان موجودا من قبل, وقم وسيلة للتنمية في هذا الموضوع مستقبلا. وجانب آخر مهم لإدخال أفكار الجبر وهو أنه سمح بتطبيق الرياضيات على نفسها بطريقة لم تحدث من قبل\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ربما كانت أحد أهم التطورات التي قامت بها الرياضيات العربية التي بدأت في هذا الوقت بعمل الخوارزمي وهي بدايات الجبر, ومن المهم فهم كيف كانت هذه الفكرة الجديدة مهمة, فقد كانت خطوة نورية بعيدا عن المفهوم اليوناني للرياضيات التي هي في جوهرها هندسة, الجبر کان نظرية موحدة تتيح الأعداد الكسرية والأعداد اللا كسرية, والمقادير الهندسية وغيرها, أن تتعامل على أنها أجسام جبرية, وأعطت الرياضيات ككل مسارا جديدا للتطور بمفهوم أوسع بكثير من الذي كان موجودا من قبل, وقم وسيلة للتنمية في هذا الموضوع مستقبلا. وجانب آخر مهم لإدخال أفكار الجبر وهو أنه سمح بتطبيق الرياضيات على نفسها بطريقة لم تحدث من قبل'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = nltk.sent_tokenize(english_text)\n",
    "arabic_sentences = nltk.sent_tokenize(arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 English paragraphs\n",
      "2 Arabic paragraphs\n"
     ]
    }
   ],
   "source": [
    "print(len(english_sentences), 'English paragraphs')\n",
    "print(len(arabic_sentences), 'Arabic paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perhaps one of the most significant advances in  made by Arabic mathematics began at this time with the work of al-Khwarizmi, namely the beginnings of algebra.',\n",
       " 'It is important to understand just how significant this new idea was.',\n",
       " 'It was a revolutionary move away from the Greek concept of mathematics which was essentially geometry.',\n",
       " 'Algebra was a unifying theory which allowedrational numbers,irrational numbers, geometrical magnitudes, etc., to all be treated as \"algebraic objects\".',\n",
       " 'It gave mathematics a whole new development path so much broader in concept to that which had existed before, and provided a vehicle for future development of the subject.',\n",
       " 'Another important aspect of the introduction of algebraic ideas was that it allowed mathematics to be applied to itselfin a way which had not happened before.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean English Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNet = WordNetLemmatizer()\n",
    "english_corpus = []\n",
    "\n",
    "for i in range(len(english_sentences)):\n",
    "    # work with only text\n",
    "    cleaning_text = re.sub('[^a-zA-Z]', ' ', english_sentences[i])\n",
    "    # text to lower case\n",
    "    cleaning_text = cleaning_text.lower()\n",
    "    # tokenize each sentence\n",
    "    cleaning_text = cleaning_text.split()\n",
    "    # lematize each word\n",
    "    sentence_lem = [WordNet.lemmatize(word) for word in cleaning_text if not word in set(stopwords.words(\"english\"))]\n",
    "    sentence = ' '.join(sentence_lem)\n",
    "    english_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perhaps one significant advance made arabic mathematics began time work al khwarizmi namely beginning algebra',\n",
       " 'important understand significant new idea',\n",
       " 'revolutionary move away greek concept mathematics essentially geometry',\n",
       " 'algebra unifying theory allowedrational number irrational number geometrical magnitude etc treated algebraic object',\n",
       " 'gave mathematics whole new development path much broader concept existed provided vehicle future development subject',\n",
       " 'another important aspect introduction algebraic idea allowed mathematics applied itselfin way happened']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNet = WordNetLemmatizer()\n",
    "arabic_corpus = []\n",
    "\n",
    "for i in range(len(arabic_sentences)):\n",
    "    # tokenize each sentence\n",
    "    cleaning_text = arabic_sentences[i].split()\n",
    "\n",
    "    # lematize each word\n",
    "    sentence_lem = [WordNet.lemmatize(word) for word in cleaning_text if not word in set(stopwords.words(\"arabic\"))]\n",
    "    sentence = ' '.join(sentence_lem)\n",
    "    arabic_corpus.append(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(english_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perhaps': 42, 'one': 40, 'significant': 45, 'advance': 0, 'made': 31, 'arabic': 8, 'mathematics': 33, 'began': 11, 'time': 48, 'work': 55, 'al': 1, 'khwarizmi': 30, 'namely': 36, 'beginning': 12, 'algebra': 2, 'important': 26, 'understand': 50, 'new': 37, 'idea': 25, 'revolutionary': 44, 'move': 34, 'away': 10, 'greek': 23, 'concept': 14, 'essentially': 16, 'geometry': 22, 'unifying': 51, 'theory': 47, 'allowedrational': 5, 'number': 38, 'irrational': 28, 'geometrical': 21, 'magnitude': 32, 'etc': 17, 'treated': 49, 'algebraic': 3, 'object': 39, 'gave': 20, 'whole': 54, 'development': 15, 'path': 41, 'much': 35, 'broader': 13, 'existed': 18, 'provided': 43, 'vehicle': 52, 'future': 19, 'subject': 46, 'another': 6, 'aspect': 9, 'introduction': 27, 'allowed': 4, 'applied': 7, 'itselfin': 29, 'way': 53, 'happened': 24}\n",
      "[2.25276297 2.25276297 1.84729786 1.84729786 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 1.84729786 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 1.84729786 1.84729786 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 1.33647224 2.25276297 2.25276297\n",
      " 2.25276297 1.84729786 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 1.84729786 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 56)\n",
      "[[0.27020314 0.27020314 0.22157044 0.         0.         0.\n",
      "  0.         0.         0.27020314 0.         0.         0.27020314\n",
      "  0.27020314 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.27020314 0.27020314 0.         0.16030048 0.         0.\n",
      "  0.27020314 0.         0.         0.         0.27020314 0.\n",
      "  0.27020314 0.         0.         0.22157044 0.         0.\n",
      "  0.27020314 0.         0.         0.         0.         0.\n",
      "  0.         0.27020314]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42690011 0.42690011 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42690011 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.42690011 0.         0.\n",
      "  0.         0.         0.5206008  0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37730799 0.\n",
      "  0.         0.         0.30939795 0.         0.37730799 0.\n",
      "  0.         0.         0.         0.         0.37730799 0.37730799\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22384142 0.37730799 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37730799 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.21650776 0.21650776 0.         0.26402925\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26402925\n",
      "  0.         0.         0.         0.26402925 0.         0.\n",
      "  0.         0.         0.         0.         0.26402925 0.\n",
      "  0.         0.         0.26402925 0.         0.         0.\n",
      "  0.         0.         0.5280585  0.26402925 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26402925\n",
      "  0.         0.26402925 0.         0.26402925 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.25240293 0.20697401 0.50480586 0.         0.\n",
      "  0.25240293 0.25240293 0.25240293 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.14974035 0.         0.25240293\n",
      "  0.         0.20697401 0.         0.         0.         0.25240293\n",
      "  0.         0.25240293 0.         0.         0.25240293 0.\n",
      "  0.         0.         0.         0.         0.25240293 0.\n",
      "  0.25240293 0.        ]\n",
      " [0.         0.         0.         0.25465267 0.31054662 0.\n",
      "  0.31054662 0.31054662 0.         0.31054662 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31054662 0.25465267 0.25465267 0.31054662 0.         0.31054662\n",
      "  0.         0.         0.         0.18423463 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31054662\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectors= vectorizer.transform(english_corpus)\n",
    "# summarize encoded vector\n",
    "print(vectors.shape)\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advance',\n",
       " 'al',\n",
       " 'algebra',\n",
       " 'algebraic',\n",
       " 'allowed',\n",
       " 'allowedrational',\n",
       " 'another',\n",
       " 'applied',\n",
       " 'arabic',\n",
       " 'aspect',\n",
       " 'away',\n",
       " 'began',\n",
       " 'beginning',\n",
       " 'broader',\n",
       " 'concept',\n",
       " 'development',\n",
       " 'essentially',\n",
       " 'etc',\n",
       " 'existed',\n",
       " 'future',\n",
       " 'gave',\n",
       " 'geometrical',\n",
       " 'geometry',\n",
       " 'greek',\n",
       " 'happened',\n",
       " 'idea',\n",
       " 'important',\n",
       " 'introduction',\n",
       " 'irrational',\n",
       " 'itselfin',\n",
       " 'khwarizmi',\n",
       " 'made',\n",
       " 'magnitude',\n",
       " 'mathematics',\n",
       " 'move',\n",
       " 'much',\n",
       " 'namely',\n",
       " 'new',\n",
       " 'number',\n",
       " 'object',\n",
       " 'one',\n",
       " 'path',\n",
       " 'perhaps',\n",
       " 'provided',\n",
       " 'revolutionary',\n",
       " 'significant',\n",
       " 'subject',\n",
       " 'theory',\n",
       " 'time',\n",
       " 'treated',\n",
       " 'understand',\n",
       " 'unifying',\n",
       " 'vehicle',\n",
       " 'way',\n",
       " 'whole',\n",
       " 'work']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advance</th>\n",
       "      <th>al</th>\n",
       "      <th>algebra</th>\n",
       "      <th>algebraic</th>\n",
       "      <th>allowed</th>\n",
       "      <th>allowedrational</th>\n",
       "      <th>another</th>\n",
       "      <th>applied</th>\n",
       "      <th>arabic</th>\n",
       "      <th>aspect</th>\n",
       "      <th>...</th>\n",
       "      <th>subject</th>\n",
       "      <th>theory</th>\n",
       "      <th>time</th>\n",
       "      <th>treated</th>\n",
       "      <th>understand</th>\n",
       "      <th>unifying</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>way</th>\n",
       "      <th>whole</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.270203</td>\n",
       "      <td>0.270203</td>\n",
       "      <td>0.221570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216508</td>\n",
       "      <td>0.216508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252403</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254653</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    advance        al   algebra  algebraic   allowed  allowedrational  \\\n",
       "0  0.270203  0.270203  0.221570   0.000000  0.000000         0.000000   \n",
       "1  0.000000  0.000000  0.000000   0.000000  0.000000         0.000000   \n",
       "2  0.000000  0.000000  0.000000   0.000000  0.000000         0.000000   \n",
       "3  0.000000  0.000000  0.216508   0.216508  0.000000         0.264029   \n",
       "4  0.000000  0.000000  0.000000   0.000000  0.000000         0.000000   \n",
       "5  0.000000  0.000000  0.000000   0.254653  0.310547         0.000000   \n",
       "\n",
       "    another   applied    arabic    aspect  ...   subject    theory      time  \\\n",
       "0  0.000000  0.000000  0.270203  0.000000  ...  0.000000  0.000000  0.270203   \n",
       "1  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.264029  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  ...  0.252403  0.000000  0.000000   \n",
       "5  0.310547  0.310547  0.000000  0.310547  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "    treated  understand  unifying   vehicle       way     whole      work  \n",
       "0  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.270203  \n",
       "1  0.000000    0.520601  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.264029    0.000000  0.264029  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000    0.000000  0.000000  0.252403  0.000000  0.252403  0.000000  \n",
       "5  0.000000    0.000000  0.000000  0.000000  0.310547  0.000000  0.000000  \n",
       "\n",
       "[6 rows x 56 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "\n",
    "- Easy to compute\n",
    "- You have some basic metric to extract the most descriptive terms in a document\n",
    "- You can easily compute the similarity between 2 documents using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages:\n",
    "- TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.\n",
    "- For this reason, TF-IDF is only useful as a lexical level feature\n",
    "- Cannot capture semantics (e.g. as compared to topic models, word embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk  : 3.5\n",
      "re    : 2.2.1\n",
      "pandas: 1.1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
